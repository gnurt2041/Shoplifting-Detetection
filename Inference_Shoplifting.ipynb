{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdZ_l4qNJFkN",
        "outputId": "6d63977c-db69-4dd1-a230-2ff00d2d6314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openmim in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.10/dist-packages (from openmim) (8.1.7)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from openmim) (0.4.6)\n",
            "Requirement already satisfied: model-index in /usr/local/lib/python3.10/dist-packages (from openmim) (0.1.11)\n",
            "Requirement already satisfied: opendatalab in /usr/local/lib/python3.10/dist-packages (from openmim) (0.0.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from openmim) (2.1.4)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.10/dist-packages (from openmim) (24.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openmim) (2.28.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from openmim) (13.4.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (6.0.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (3.7)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.10/dist-packages (from model-index->openmim) (4.1.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (3.20.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (4.65.2)\n",
            "Requirement already satisfied: openxlab in /usr/local/lib/python3.10/dist-packages (from opendatalab->openmim) (0.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->openmim) (2024.8.30)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->openmim) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->openmim) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.16.0)\n",
            "Requirement already satisfied: filelock~=3.14.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (3.14.0)\n",
            "Requirement already satisfied: oss2~=2.17.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (2.17.0)\n",
            "Requirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (24.1)\n",
            "Requirement already satisfied: setuptools~=60.2.0 in /usr/local/lib/python3.10/dist-packages (from openxlab->opendatalab->openmim) (60.2.0)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.10/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (1.7)\n",
            "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.5)\n",
            "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.10/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.15.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (0.10.0)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (43.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.22)\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu121/torch2.4.0/index.html\n",
            "Collecting mmengine\n",
            "  Downloading mmengine-0.10.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting addict (from mmengine)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmengine) (1.26.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmengine) (6.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine) (2.4.0)\n",
            "Collecting yapf (from mmengine)\n",
            "  Downloading yapf-0.40.2-py3-none-any.whl.metadata (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmengine) (4.10.0.84)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine) (2.18.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (8.5.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmengine) (2.0.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmengine) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine) (1.16.0)\n",
            "Downloading mmengine-0.10.5-py3-none-any.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.3/452.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: addict, yapf, mmengine\n",
            "Successfully installed addict-2.4.0 mmengine-0.10.5 yapf-0.40.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U openmim\n",
        "!mim install mmengine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUizCsuHJGfo",
        "outputId": "7e7f017e-9707-42e9-c601-bfe1a31098ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mmcv==2.1.0\n",
            "  Using cached mmcv-2.1.0.tar.gz (471 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (2.4.0)\n",
            "Requirement already satisfied: mmengine>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (0.10.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (24.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (10.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (6.0.2)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from mmcv==2.1.0) (0.40.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv==2.1.0) (3.7.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv==2.1.0) (13.4.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv==2.1.0) (2.4.0)\n",
            "Requirement already satisfied: opencv-python>=3 in /usr/local/lib/python3.10/dist-packages (from mmengine>=0.3.0->mmcv==2.1.0) (4.10.0.84)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==2.1.0) (8.5.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==2.1.0) (4.3.6)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->mmcv==2.1.0) (2.0.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->mmcv==2.1.0) (3.20.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv==2.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->mmengine>=0.3.0->mmcv==2.1.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->mmengine>=0.3.0->mmcv==2.1.0) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mmengine>=0.3.0->mmcv==2.1.0) (1.16.0)\n",
            "Building wheels for collected packages: mmcv\n"
          ]
        }
      ],
      "source": [
        "!pip install mmcv==2.1.0\n",
        "!mim install \"mmdet==3.2.0\"\n",
        "!mim install \"mmpose>=1.1.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtAZaoIrJH7t"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/open-mmlab/mmpose.git\n",
        "%cd mmpose\n",
        "!pip install -r requirements.txt\n",
        "!pip install -v -e .\n",
        "# \"-v\" means verbose, or more output\n",
        "# \"-e\" means installing a project in editable mode,\n",
        "# thus any local modifications made to the code will take effect without reinstallation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNUyLgDhJJMR"
      },
      "outputs": [],
      "source": [
        "!wget https://download.openmmlab.com/mmpose/v1/projects/rtmo/rtmo-m_16xb16-600e_coco-640x640-6f4e0306_20231211.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4jlRlAXJK_M"
      },
      "outputs": [],
      "source": [
        "%cd ..\n",
        "!git clone https://github.com/ifzhang/ByteTrack.git\n",
        "%cd ByteTrack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPh8j2WHJL71"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q -r requirements.txt\n",
        "!python3 setup.py -q develop\n",
        "!pip install -q cython_bbox onemetric loguru lap thop==0.1.1.post2209072238\n",
        "from IPython import display\n",
        "display.clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "wGfOs74-JM2u"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/ByteTrack')\n",
        "sys.path.append('/content/mmpose')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IDh2y8J3JRAz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/teamspace/studios/this_studio/mmpose\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd mmpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uMrgYzz7Jihk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from typing import Optional, List, Dict\n",
        "from dataclasses import dataclass\n",
        "from yolox.tracker.byte_tracker import STrack, BYTETracker\n",
        "from onemetric.cv.utils.iou import box_iou_batch\n",
        "from mmpose.apis import MMPoseInferencer\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "np.float = np.float64\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kpFln1hDJfCL"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Detection:\n",
        "    x_min: float\n",
        "    y_min: float\n",
        "    x_max: float\n",
        "    y_max: float\n",
        "    keypoints: np.ndarray\n",
        "    keypoint_scores: np.ndarray\n",
        "    confidence: float\n",
        "    tracker_id: Optional[int] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_numpy(cls, pred: np.ndarray, keypoints_list: List[np.ndarray], keypoint_scores_list: List[np.ndarray]):\n",
        "        result = []\n",
        "        for (x_min, y_min, x_max, y_max, confidence), keypoints, keypoint_scores in zip(pred, keypoints_list, keypoint_scores_list):\n",
        "            result.append(Detection(\n",
        "                x_min = float(x_min),\n",
        "                y_min = float(y_min),\n",
        "                x_max = float(x_max),\n",
        "                y_max = float(y_max),\n",
        "                keypoints = keypoints,\n",
        "                keypoint_scores = keypoint_scores,\n",
        "                confidence=float(confidence)\n",
        "            ))\n",
        "        return result\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BYTETrackerArgs:\n",
        "    track_thresh: float = 0.25\n",
        "    track_buffer: int = 30\n",
        "    match_thresh: float = 0.8\n",
        "    aspect_ratio_thresh: float = 3.0\n",
        "    min_box_area: float = 1.0\n",
        "    mot20: bool = False\n",
        "\n",
        "# converts List into format that can be consumed by match_detections_with_tracks function\n",
        "def detections2boxes(detections: List[Detection], with_confidence: bool = True) -> np.ndarray:\n",
        "    return np.array([\n",
        "        [\n",
        "            detection.x_min,\n",
        "            detection.y_min,\n",
        "            detection.x_max,\n",
        "            detection.y_max,\n",
        "            detection.confidence\n",
        "        ] if with_confidence else [\n",
        "            detection.x_min,\n",
        "            detection.y_min,\n",
        "            detection.x_max,\n",
        "            detection.y_max,\n",
        "        ]\n",
        "        for detection\n",
        "        in detections\n",
        "    ], dtype=float)\n",
        "def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n",
        "    return np.array([\n",
        "        track.tlbr\n",
        "        for track\n",
        "        in tracks\n",
        "    ], dtype=np.float64)\n",
        "\n",
        "def match_detections_with_tracks(\n",
        "    detections: List[Detection],\n",
        "    tracks: List[STrack]\n",
        ") -> List:\n",
        "    if not np.any(detections) or len(tracks) == 0:\n",
        "        return np.empty((0,))\n",
        "    detection_boxes = detections2boxes(detections=detections, with_confidence=False)\n",
        "    tracks_boxes = tracks2boxes(tracks=tracks)\n",
        "    iou = box_iou_batch(tracks_boxes, detection_boxes)\n",
        "    track2detection = np.argmax(iou, axis=1)\n",
        "\n",
        "    for tracker_index, detection_index in enumerate(track2detection):\n",
        "        if iou[tracker_index, detection_index] != 0:\n",
        "            detections[detection_index].tracker_id = tracks[tracker_index].track_id\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jE5JowmLHMoy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_joint_heatmaps(keypoints, heatmap_size, sigma=0.6):\n",
        "    \"\"\"\n",
        "    Generate a heatmap for each keypoint and stack them into a 3D volume (KxHxW).\n",
        "\n",
        "    Args:\n",
        "    - keypoints: numpy array of shape (K, 3), where K is the number of keypoints,\n",
        "                 and each keypoint is represented as (x, y, c) for coordinates and confidence score.\n",
        "    - heatmap_size: tuple (H, W), the size of each heatmap (height, width).\n",
        "    - sigma: standard deviation (spread) of the Gaussian.\n",
        "\n",
        "    Returns:\n",
        "    - heatmaps: numpy array of shape (K, H, W), the stacked 3D heatmaps.\n",
        "    \"\"\"\n",
        "    K = keypoints.shape[0]\n",
        "    H, W = heatmap_size\n",
        "\n",
        "    # Initialize the 3D heatmap volume: shape (K, H, W)\n",
        "    heatmaps = np.zeros((K, H, W), dtype=np.float32)\n",
        "\n",
        "    # Create a meshgrid for the heatmap\n",
        "    x = np.arange(0, W, 1)\n",
        "    y = np.arange(0, H, 1)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "\n",
        "    # Generate a heatmap for each keypoint\n",
        "    for k in range(K):\n",
        "        xk, yk, ck = keypoints[k]  # Extract (x, y, confidence) for keypoint k\n",
        "        # Apply the Gaussian formula from the equation\n",
        "        gaussian_map = np.exp(-((xx - xk) ** 2 + (yy - yk) ** 2) / (2 * sigma ** 2)) * ck\n",
        "        heatmaps[k] = gaussian_map  # Store the heatmap for keypoint k\n",
        "\n",
        "    return heatmaps\n",
        "\n",
        "def distance_to_segment(px, py, ax, ay, bx, by):\n",
        "    \"\"\"\n",
        "    Compute the distance from point (px, py) to the line segment between (ax, ay) and (bx, by).\n",
        "    Vectorized operation for entire heatmap at once.\n",
        "    \"\"\"\n",
        "    # Vectorize the line segment length\n",
        "    segment_length_sq = (bx - ax) ** 2 + (by - ay) ** 2\n",
        "\n",
        "    # Prevent division by zero in case a and b are the same points\n",
        "    segment_length_sq = np.maximum(segment_length_sq, 1e-8)\n",
        "\n",
        "    # Projection of the point onto the line segment\n",
        "    t = np.clip(((px - ax) * (bx - ax) + (py - ay) * (by - ay)) / segment_length_sq, 0, 1)\n",
        "\n",
        "    # Projection coordinates on the line segment\n",
        "    proj_x = ax + t * (bx - ax)\n",
        "    proj_y = ay + t * (by - ay)\n",
        "\n",
        "    # Distance from point (px, py) to the projection\n",
        "    return np.sqrt((px - proj_x) ** 2 + (py - proj_y) ** 2)\n",
        "\n",
        "def generate_limb_heatmaps(limb_pairs, heatmap_size, sigma=0.6):\n",
        "    \"\"\"\n",
        "    Generate limb heatmaps for all limb pairs and stack them into a 3D volume (KxHxW).\n",
        "\n",
        "    Args:\n",
        "    - limb_pairs: numpy array of shape (K, 2, 3), where K is the number of limbs,\n",
        "                  and each limb is represented by two keypoints ((x_a, y_a, c_a), (x_b, y_b, c_b)).\n",
        "    - heatmap_size: tuple (H, W), the size of each heatmap (height, width).\n",
        "    - sigma: standard deviation (spread) of the Gaussian.\n",
        "\n",
        "    Returns:\n",
        "    - heatmaps: numpy array of shape (K, H, W), the stacked 3D heatmaps for limbs.\n",
        "    \"\"\"\n",
        "    K = limb_pairs.shape[0]\n",
        "    H, W = heatmap_size\n",
        "\n",
        "    # Initialize the 3D heatmap volume: shape (K, H, W)\n",
        "    heatmaps = np.zeros((K, H, W), dtype=np.float32)\n",
        "\n",
        "    # Create a meshgrid for the heatmap\n",
        "    x = np.arange(0, W, 1)\n",
        "    y = np.arange(0, H, 1)\n",
        "    xx, yy = np.meshgrid(x, y)\n",
        "\n",
        "    for k in range(K):\n",
        "        # Extract coordinates and confidence scores for the two keypoints forming the limb\n",
        "        (xa, ya, ca), (xb, yb, cb) = limb_pairs[k]\n",
        "\n",
        "        # Compute the distance of each point in the heatmap to the limb segment\n",
        "        distances = distance_to_segment(xx, yy, xa, ya, xb, yb)\n",
        "\n",
        "        # Apply the Gaussian formula\n",
        "        gaussian_map = np.exp(-distances ** 2 / (2 * sigma ** 2))\n",
        "\n",
        "        # Multiply by the minimum confidence between the two keypoints\n",
        "        confidence = min(ca, cb)\n",
        "        heatmaps[k] = gaussian_map * confidence\n",
        "\n",
        "    return heatmaps\n",
        "\n",
        "def generate_limb_pairs(keypoints, connections):\n",
        "    \"\"\"\n",
        "    Generate limb pairs for each connection of keypoints.\n",
        "\n",
        "    Args:\n",
        "    - keypoints: numpy array of shape (K, 3), where K is the number of keypoints,\n",
        "                 and each keypoint is represented as (x, y, c).\n",
        "    - connections: list of tuples, where each tuple contains two indices representing a connection\n",
        "                   between two keypoints (e.g., (a, b)).\n",
        "\n",
        "    Returns:\n",
        "    - limb_pairs: numpy array of shape (L, 2, 3), where L is the number of limbs,\n",
        "                  and each limb contains two keypoints represented as (x, y, c).\n",
        "    \"\"\"\n",
        "    limb_pairs = []\n",
        "\n",
        "    for (a, b) in connections:\n",
        "        if a < keypoints.shape[0] and b < keypoints.shape[0]:\n",
        "            # Extract the keypoints corresponding to the connection\n",
        "            point_a = keypoints[a]\n",
        "            point_b = keypoints[b]\n",
        "            limb_pairs.append([point_a, point_b])\n",
        "\n",
        "    return np.array(limb_pairs)\n",
        "\n",
        "byte_tracker = BYTETracker(BYTETrackerArgs())\n",
        "connections = [\n",
        "    (0, 1), (0, 2), (2, 4), (1, 3), (0,5), (0,6), (5,7), (7,9), (6,8), (8,10), (5,11),\n",
        "    (6,12), (11,12), (12,14), (11,13), (13,15), (14,16)\n",
        "]\n",
        "\n",
        "def extract_keypoints_and_create_heatmaps(inferencer, video_path, heatmap_size):\n",
        "    \"\"\"Extract keypoints from video and generate heatmaps.\"\"\"\n",
        "    result_generator = inferencer(video_path, show=False)\n",
        "    results = [result for result in result_generator]\n",
        "    dict_person = {}\n",
        "    vcap = cv2.VideoCapture(video_path)\n",
        "    width = vcap.get(3)\n",
        "    height = vcap.get(4)\n",
        "    frame_shape = (width,height)\n",
        "    for frame_id, result in enumerate(results):\n",
        "      bboxs = np.array([])\n",
        "      keypoints_list = []\n",
        "      keypoint_scores_list = []\n",
        "      # frame_draw = Image.fromarray(frame)\n",
        "      if len(result['predictions'][0]) != 0:\n",
        "            for i, res in enumerate(result['predictions'][0]):\n",
        "              # bounding box are xmin, ymin, xmax, ymax\n",
        "              if res['bbox_score'] > 0.45 and np.mean(res['keypoint_scores']) > 0.45:\n",
        "                  box = np.array(res['bbox'][0],dtype = np.float32)\n",
        "                  # crop_frame = frame[int(box[1]) : int(box[3]), int(box[0]) : int(box[2])]\n",
        "                  if len(bboxs) == 0:\n",
        "                      box = np.append(box,res['bbox_score'])\n",
        "                      bboxs = box\n",
        "                  else:\n",
        "                      box = np.append(box,res['bbox_score'])\n",
        "                      bboxs = np.vstack((bboxs,box))\n",
        "                  keypoints = res['keypoints']\n",
        "                  adjusted_keypoints = []\n",
        "                  for point in keypoints:\n",
        "                      adjusted_keypoints.append([point[0], point[1]])\n",
        "                  keypoints_list.append(np.array(adjusted_keypoints))\n",
        "                  keypoint_scores_list.append(res['keypoint_scores'])\n",
        "            if len(bboxs) > 0:\n",
        "              if bboxs.ndim == 1:\n",
        "                  bboxs = np.expand_dims(bboxs,0)\n",
        "              detections = Detection.from_numpy(pred=bboxs.copy(), keypoints_list = keypoints_list.copy(), keypoint_scores_list=keypoint_scores_list)\n",
        "              tracks = byte_tracker.update(\n",
        "                      output_results=bboxs,\n",
        "                      img_info=frame_shape,\n",
        "                      img_size=frame_shape\n",
        "              )\n",
        "              tracked_detections = match_detections_with_tracks(detections=detections.copy(), tracks=tracks.copy())\n",
        "              detection_boxes = detections2boxes(detections=tracked_detections, with_confidence=False)\n",
        "              if len(tracked_detections) > 0:\n",
        "                  for i, tracked_detection in enumerate(tracked_detections):\n",
        "                      if tracked_detection.tracker_id:\n",
        "                          # Generate joint heatmaps and limb heatmaps\n",
        "                          keypoints_w_conf = tracked_detection.keypoints.copy()\n",
        "                          keypoints_w_conf = np.append(keypoints_w_conf, np.array(tracked_detection.keypoint_scores).reshape(-1,1),1)\n",
        "                          joint_heatmap = generate_joint_heatmaps(keypoints_w_conf, heatmap_size)\n",
        "\n",
        "                          limb_pairs = generate_limb_pairs(keypoints_w_conf, connections)\n",
        "                          limb_heatmap = generate_limb_heatmaps(limb_pairs, heatmap_size)\n",
        "                          if tracked_detection.tracker_id not in dict_person:\n",
        "                              # Initialize the joint and limb heatmaps as np.array with shape (1, 17, 56, 56)\n",
        "                              dict_person[tracked_detection.tracker_id] = {\n",
        "                                  'joint_heatmaps': joint_heatmap[np.newaxis, ...],  # Shape (1, 17, 56, 56)\n",
        "                                  'limb_heatmaps': limb_heatmap[np.newaxis, ...],    # Shape (1, 17, 56, 56)\n",
        "                                  'bboxs': [[tracked_detection.x_min, tracked_detection.y_min, tracked_detection.x_max, tracked_detection.y_max]],  # Store bbox as list\n",
        "                                  'keypoints': [tracked_detection.keypoints],         # List of keypoints for each frame\n",
        "                                  'frame_ids': [frame_id]  # Store current frame ID\n",
        "                              }\n",
        "                          else:\n",
        "                              # Concatenate the new heatmap along the time axis (axis=0)\n",
        "                              dict_person[tracked_detection.tracker_id]['joint_heatmaps'] = np.concatenate(\n",
        "                                  (dict_person[tracked_detection.tracker_id]['joint_heatmaps'], joint_heatmap[np.newaxis, ...]), axis=0\n",
        "                              )\n",
        "                              dict_person[tracked_detection.tracker_id]['limb_heatmaps'] = np.concatenate(\n",
        "                                  (dict_person[tracked_detection.tracker_id]['limb_heatmaps'], limb_heatmap[np.newaxis, ...]), axis=0\n",
        "                              )\n",
        "                              dict_person[tracked_detection.tracker_id]['bboxs'].append([tracked_detection.x_min, tracked_detection.y_min, tracked_detection.x_max, tracked_detection.y_max])\n",
        "                              dict_person[tracked_detection.tracker_id]['keypoints'].append(tracked_detection.keypoints)\n",
        "                              dict_person[tracked_detection.tracker_id]['frame_ids'].append(frame_id)\n",
        "\n",
        "    return dict_person\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# def uniform_sample_clip(video, T=48):\n",
        "#     \"\"\"\n",
        "#     Uniformly sample T frames from a video by dividing the video into T segments\n",
        "#     and randomly selecting one frame from each segment.\n",
        "\n",
        "#     Args:\n",
        "#         video (torch.Tensor): Tensor containing video frames with shape (Total_frames, C, H, W).\n",
        "#         T (int): Number of frames to sample (i.e., the number of segments to divide the video into).\n",
        "\n",
        "#     Returns:\n",
        "#         torch.Tensor: A tensor of shape (T, C, H, W) containing the sampled frames.\n",
        "#     \"\"\"\n",
        "#     # Total number of frames in the video\n",
        "#     total_frames = video.shape[0]\n",
        "\n",
        "#     # Check if T is valid\n",
        "#     if T <= 0 or T > total_frames:\n",
        "#         raise ValueError(f\"T must be between 1 and {total_frames}, but got T={T}.\")\n",
        "\n",
        "#     # Calculate the length of each segment (using integer division)\n",
        "#     segment_length = total_frames // T\n",
        "\n",
        "#     sampled_frames = []\n",
        "\n",
        "#     # Sample one random frame from each segment\n",
        "#     for i in range(T):\n",
        "#         start = i * segment_length\n",
        "#         end = (i + 1) * segment_length if i < T - 1 else total_frames  # Last segment goes to total_frames\n",
        "\n",
        "#         # Randomly choose a frame within this segment\n",
        "#         random_index = torch.randint(start, end, (1,)).item()\n",
        "\n",
        "#         # Append the selected frame to the result list\n",
        "#         sampled_frames.append(video[random_index])\n",
        "\n",
        "#     # Stack the sampled frames into a tensor of shape (T, C, H, W)\n",
        "#     sampled_clip = np.array(sampled_frames)\n",
        "\n",
        "#     return sampled_clip\n",
        "\n",
        "def uniform_sample_clip(video, T):\n",
        "    \"\"\"\n",
        "    Uniformly sample T frames from a video by dividing the video into T segments\n",
        "    and randomly selecting one frame from each segment.\n",
        "\n",
        "    Args:\n",
        "        video (torch.Tensor): Tensor containing video frames with shape (Total_frames, C, H, W).\n",
        "        T (int): Number of frames to sample (i.e., the number of segments to divide the video into).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A tensor of shape (T, C, H, W) containing the sampled frames.\n",
        "    \"\"\"\n",
        "    # Total number of frames in the video\n",
        "    total_frames = video.shape[0]\n",
        "\n",
        "    # Calculate the length of each segment\n",
        "    segment_length = total_frames / T\n",
        "\n",
        "    sampled_frames = []\n",
        "\n",
        "    # Sample one random frame from each segment\n",
        "    for i in range(T):\n",
        "        start = int(i * segment_length)\n",
        "        end = int((i + 1) * segment_length)\n",
        "\n",
        "        # Randomly choose a frame within this segment\n",
        "        random_index = torch.randint(start, end, (1,)).item()\n",
        "\n",
        "        # Append the selected frame to the result list\n",
        "        sampled_frames.append(video[random_index])\n",
        "\n",
        "    # Stack the sampled frames into a tensor of shape (T, C, H, W)\n",
        "    sampled_clip = torch.stack(sampled_frames)\n",
        "\n",
        "    return sampled_clip\n",
        "\n",
        "def sample_person_heatmaps(dict_person, min_T = 32, T=48, scale=6, method='duplicate'):\n",
        "    \"\"\"\n",
        "    Uniformly sample 48 heatmaps from every 288 frames for each person in dict_person.\n",
        "    \n",
        "    Args:\n",
        "        dict_person (dict): Dictionary containing heatmaps for each tracked person.\n",
        "        T (int): Number of frames to sample.\n",
        "        scale (int): The scale factor for frame sampling.\n",
        "        method (str): Method to use when frames are less than T: 'duplicate' or 'interpolate'.\n",
        "        \n",
        "    Returns:\n",
        "        dict: A new dictionary with sampled heatmaps of shape (T, 17, 56, 56).\n",
        "    \"\"\"\n",
        "    sampled_dict_person = {}\n",
        "    total_frames_threshold = int(T * scale)\n",
        "\n",
        "    for person_id, data in dict_person.items():\n",
        "        # Get the joint and limb heatmaps\n",
        "        joint_heatmaps = data['joint_heatmaps']  # Shape (Total_frames, 17, 56, 56)\n",
        "        limb_heatmaps = data['limb_heatmaps']    # Shape (Total_frames, 17, 56, 56)\n",
        "        total_frames = joint_heatmaps.shape[0]\n",
        "\n",
        "        # Prepare lists to store sampled heatmaps\n",
        "        sampled_joint_heatmaps = []\n",
        "        sampled_limb_heatmaps = []\n",
        "        batch_joint_heatmaps = np.array([])\n",
        "\n",
        "        # Process frames in batches of 288\n",
        "        if scale == 0 or total_frames_threshold >= total_frames:\n",
        "            if total_frames >= T:\n",
        "                sampled_joint = uniform_sample_clip(torch.tensor(joint_heatmaps), T)\n",
        "                sampled_limb = uniform_sample_clip(torch.tensor(limb_heatmaps), T)\n",
        "            elif total_frames > min_T:\n",
        "                if method == 'duplicate':\n",
        "                    # Duplicate frames to make up the difference\n",
        "                    sampled_joint = np.resize(joint_heatmaps, (T, *joint_heatmaps.shape[1:]))\n",
        "                    sampled_limb = np.resize(limb_heatmaps, (T, *limb_heatmaps.shape[1:]))\n",
        "                elif method == 'interpolate':\n",
        "                    # Interpolate between frames\n",
        "                    sampled_joint = interpolate_heatmaps(joint_heatmaps, T)\n",
        "                    sampled_limb = interpolate_heatmaps(limb_heatmaps, T)\n",
        "            elif total_frames < min_T:\n",
        "                    sampled_joint = np.array([])\n",
        "                    sampled_limb = np.array([])\n",
        "        else:\n",
        "            for start in range(0, total_frames, total_frames_threshold):\n",
        "                end = min(start + total_frames_threshold, total_frames)  # Handle remaining frames\n",
        "                batch_joint_heatmaps = joint_heatmaps[start:end]\n",
        "                batch_limb_heatmaps = limb_heatmaps[start:end]\n",
        "\n",
        "                # If the batch has enough frames, sample uniformly\n",
        "                if batch_joint_heatmaps.shape[0] >= total_frames_threshold or batch_joint_heatmaps.shape[0] >= T:\n",
        "                    # Use the uniform_sample_clip function to sample T frames from this batch\n",
        "                    sampled_joint = uniform_sample_clip(torch.tensor(batch_joint_heatmaps), T)\n",
        "                    sampled_limb = uniform_sample_clip(torch.tensor(batch_limb_heatmaps), T)\n",
        "                elif batch_joint_heatmaps.shape[0] > min_T:\n",
        "                    # If not enough frames, handle based on the chosen method\n",
        "                    if method == 'duplicate':\n",
        "                        # Duplicate frames to make up the difference\n",
        "                        sampled_joint = np.resize(batch_joint_heatmaps, (T, *batch_joint_heatmaps.shape[1:]))\n",
        "                        sampled_limb = np.resize(batch_limb_heatmaps, (T, *batch_limb_heatmaps.shape[1:]))\n",
        "                    elif method == 'interpolate':\n",
        "                        # Interpolate between frames\n",
        "                        sampled_joint = interpolate_heatmaps(batch_joint_heatmaps, T)\n",
        "                        sampled_limb = interpolate_heatmaps(batch_limb_heatmaps, T)\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown method '{method}'. Use 'duplicate' or 'interpolate'.\")\n",
        "                else:\n",
        "                    sampled_joint = np.array([])\n",
        "                    sampled_limb = np.array([])\n",
        "        if len(sampled_joint) > 0:\n",
        "            # Append to sampled lists\n",
        "            if len(batch_joint_heatmaps)>0:\n",
        "                print(batch_joint_heatmaps.shape,'->',sampled_joint.shape)\n",
        "            else:\n",
        "                print(joint_heatmaps.shape,'->',sampled_joint.shape)\n",
        "            sampled_joint_heatmaps.append(sampled_joint)\n",
        "            sampled_limb_heatmaps.append(sampled_limb)\n",
        "\n",
        "        # Concatenate all sampled heatmaps\n",
        "        # sampled_joint_heatmaps = np.concatenate(sampled_joint_heatmaps, axis=0)\n",
        "        # sampled_limb_heatmaps = np.concatenate(sampled_limb_heatmaps, axis=0)\n",
        "        \n",
        "        # Copy the bbox and keypoints as they don't need sampling\n",
        "        bboxs = data['bboxs']\n",
        "        keypoints = data['keypoints']\n",
        "        frame_ids = data['frame_ids']  # Store current frame ID\n",
        "\n",
        "        # Store the sampled heatmaps and other data in a new dictionary\n",
        "        sampled_dict_person[person_id] = {\n",
        "            'joint_heatmaps': sampled_joint_heatmaps,  # Already in numpy format\n",
        "            'limb_heatmaps': sampled_limb_heatmaps,    # Already in numpy format\n",
        "            'bboxs': bboxs,  # Keep bboxs and keypoints unchanged\n",
        "            'keypoints': keypoints,\n",
        "            'frame_ids': frame_ids\n",
        "        }\n",
        "\n",
        "    return sampled_dict_person\n",
        "\n",
        "def interpolate_heatmaps(heatmaps, target_frames):\n",
        "    \"\"\"\n",
        "    Interpolates heatmaps to match the target number of frames.\n",
        "    \n",
        "    Args:\n",
        "        heatmaps (np.ndarray): Heatmaps of shape (frames, 17, 56, 56).\n",
        "        target_frames (int): The desired number of frames.\n",
        "        \n",
        "    Returns:\n",
        "        np.ndarray: Interpolated heatmaps of shape (target_frames, 17, 56, 56).\n",
        "    \"\"\"\n",
        "    current_frames = heatmaps.shape[0]\n",
        "    factor = target_frames / current_frames\n",
        "    interpolated_heatmaps = np.zeros((target_frames, *heatmaps.shape[1:]))\n",
        "\n",
        "    for i in range(target_frames):\n",
        "        # Map new frame to old frame indices\n",
        "        old_frame_idx = i / factor\n",
        "        lower_idx = int(np.floor(old_frame_idx))\n",
        "        upper_idx = min(int(np.ceil(old_frame_idx)), current_frames - 1)\n",
        "\n",
        "        # Linear interpolation between two closest frames\n",
        "        weight = old_frame_idx - lower_idx\n",
        "        interpolated_heatmaps[i] = (1 - weight) * heatmaps[lower_idx] + weight * heatmaps[upper_idx]\n",
        "\n",
        "    return interpolated_heatmaps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "wAeJ7oKQGubL"
      },
      "outputs": [],
      "source": [
        "def run_inference(heatmaps, model):\n",
        "    \"\"\"Run inference on the heatmaps using the 3D-CNN model.\"\"\"\n",
        "    # heatmaps_tensor = torch.tensor(heatmaps).unsqueeze(0)  # Add batch dim (1, 17, T, 56, 56)\n",
        "    # print(heatmaps_tensor.shape)\n",
        "    heatmaps_tensor = torch.tensor(heatmaps,dtype=torch.float)\n",
        "    heatmaps_tensor = heatmaps_tensor.permute(1,0,2,3).unsqueeze(0)\n",
        "    # print(heatmaps_tensor.shape)\n",
        "    with torch.no_grad():\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        output = model(heatmaps_tensor)\n",
        "        output = output.squeeze(0).cpu().numpy()  # Remove batch dim and move to CPU\n",
        "        print(output)\n",
        "        output = np.argmax(output, axis=0).astype(int)  # Get the class with highest probability\n",
        "    label = 'Normal' if output == 0 else 'Shoplifting'\n",
        "    return label\n",
        "\n",
        "def run_inference_on_sampled_dict(sampled_dict_person, model, heatmap_type='joint'):\n",
        "    \"\"\"\n",
        "    Run inference on the sampled heatmaps for each person in sampled_dict_person and\n",
        "    include bboxs and keypoints in the results.\n",
        "\n",
        "    Args:\n",
        "        sampled_dict_person (dict): Dictionary containing the sampled heatmaps for each tracked person.\n",
        "        model: The 3D-CNN model for inference.\n",
        "        heatmap_type (str): The type of heatmaps to use for inference. Can be 'joint', 'limb', or 'both'.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with the inference results, bboxs, and keypoints for each person.\n",
        "    \"\"\"\n",
        "    inference_results = {}\n",
        "\n",
        "    for person_id, data in sampled_dict_person.items():\n",
        "        # Initialize a list to hold predictions for each clip\n",
        "        predictions = []\n",
        "\n",
        "        # Select the appropriate heatmaps based on heatmap_type\n",
        "        if heatmap_type == 'joint':\n",
        "            heatmaps_list = data['joint_heatmaps']\n",
        "        elif heatmap_type == 'limb':\n",
        "            heatmaps_list = data['limb_heatmaps']\n",
        "        elif heatmap_type == 'both':\n",
        "            # Combine corresponding clips of joint and limb heatmaps\n",
        "            heatmaps_list = [j + l for j, l in zip(data['joint_heatmaps'], data['limb_heatmaps'])]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid heatmap_type. Must be 'joint', 'limb', or 'both'.\")\n",
        "\n",
        "        # Run inference on each heatmap clip and collect predictions\n",
        "        for heatmaps in heatmaps_list:\n",
        "            prediction = run_inference(heatmaps, model)  # Each heatmap is of shape (T, 17, 56, 56)\n",
        "            # prediction = [prediction]*heatmaps.shape[0]\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        # Store the result along with bbox and keypoints\n",
        "        if len(predictions) > 0:\n",
        "            inference_results[person_id] = {\n",
        "                'predictions': predictions,  # List of predictions for each T-frame clip\n",
        "                'bboxs': data['bboxs'],\n",
        "                'keypoints': data['keypoints'],\n",
        "                'frame_ids': data['frame_ids']\n",
        "            }\n",
        "\n",
        "    return inference_results\n",
        "\n",
        "def visualize_inference_on_video(video_path, inference_results, output_path='prediction.mp4', fps=30, T = 48, scale = 6):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    frame_selected = int(T*scale)\n",
        "    if scale == 0:\n",
        "        frame_selected = total_frames\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break  # End of video\n",
        "\n",
        "        # Check for each person in the inference results\n",
        "        for person_id, result in inference_results.items():\n",
        "            # Check if there are predictions for this person\n",
        "            if frame_count in result['frame_ids']:\n",
        "                # Find the current clip index based on frame_count\n",
        "                clip_index = frame_count // frame_selected\n",
        "                bbox_id = result['frame_ids'].index(frame_count)\n",
        "\n",
        "                # Ensure clip_index does not exceed available predictions\n",
        "                if clip_index < len(result['predictions']):\n",
        "                    prediction = result['predictions'][clip_index]  # Get the prediction for the current clip\n",
        "                    bbox = result['bboxs'][bbox_id]  # Get the corresponding bbox for this clip\n",
        "                    keypoints = result['keypoints'][bbox_id]  # Get the corresponding keypoints\n",
        "\n",
        "                    # Determine label and color\n",
        "                    color = (0, 255, 0) if prediction == 'Normal' else (0, 0, 255) \n",
        "\n",
        "                    # Draw bounding box\n",
        "                    x_min, y_min, x_max, y_max = bbox\n",
        "                    cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), color, 2)\n",
        "                    cv2.putText(frame, f'{prediction}',\n",
        "                                (int(x_min), int(y_min) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "                    # Draw keypoints (if needed)\n",
        "                    # for (x, y) in keypoints:  # Assuming keypoints are available and valid\n",
        "                    #     cv2.circle(frame, (int(x), int(y)), 3, color, -1)\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    # cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "eNpjlO20STss"
      },
      "outputs": [],
      "source": [
        "def process_input(model=None, video_source=None, inferencer=None, heatmap_size=(56, 56), heatmap_type='joint', \n",
        "                visualize=False, output_path='output_video.mp4', fps=30, min_T=32, T=64, scale=5, method='interpolate'):\n",
        "    \"\"\"\n",
        "    Process input for inference.\n",
        "    If video_source is provided and valid, it processes the video file.\n",
        "    If video_source is None or 0, it processes live camera stream.\n",
        "\n",
        "    Args:\n",
        "        model: The 3D-CNN model for inference.\n",
        "        video_source: The source of the video (0 for camera, filename for video).\n",
        "        inferencer: The object or function to extract keypoints and create heatmaps.\n",
        "        heatmap_size: The size of the heatmaps.\n",
        "        heatmap_type: Type of heatmap to process ('joint', 'limb', or 'both').\n",
        "        visualize: Flag to indicate whether to visualize the output.\n",
        "        output_path: The path to save the output video if visualizing.\n",
        "    \"\"\"\n",
        "    if video_source is None or video_source == 0:\n",
        "        # Real-time inference from camera\n",
        "        cap = cv2.VideoCapture(0)  # Camera stream\n",
        "        frames = []\n",
        "        dict_person = {}\n",
        "\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) >= 48:\n",
        "                # Process frames to extract heatmaps\n",
        "                dict_person = extract_keypoints_and_create_heatmaps(inferencer, video_source, heatmap_size)\n",
        "                dict_person = sample_person_heatmaps(dict_person, min_T, T,scale,method)\n",
        "\n",
        "                # Run inference\n",
        "                dict_pred = run_inference_on_sampled_dict(dict_person, model, heatmap_type)\n",
        "\n",
        "                # Print predictions for each person\n",
        "                for person_id, prediction in dict_pred.items():\n",
        "                    print(f\"Person ID {person_id}: Prediction: {dict_pred[person_id]['predictions']}\")\n",
        "\n",
        "                # Visualization\n",
        "                if visualize:\n",
        "                    visualize_inference_on_video(None, dict_person, output_path,fps,T,scale)  # Adjust as necessary\n",
        "\n",
        "                frames = []  # Reset for next batch\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "    else:\n",
        "        # Process video file\n",
        "        print(f'--------Extracting keypoints and generate heatmap for {heatmap_type}--------')\n",
        "        dict_person = extract_keypoints_and_create_heatmaps(inferencer, video_source, heatmap_size)\n",
        "        dict_person = sample_person_heatmaps(dict_person, min_T, T,scale,method)\n",
        "        print(f'--------Runing inference on heatmap--------')\n",
        "        # Run inference\n",
        "        dict_pred = run_inference_on_sampled_dict(dict_person, model, heatmap_type)\n",
        "        print(f'--------Prediction--------')\n",
        "        # Print predictions for each person\n",
        "        for person_id, prediction in dict_pred.items():\n",
        "            print(f\"Person ID {person_id} -> Prediction: {dict_pred[person_id]['predictions']}\")\n",
        "\n",
        "        # Visualization\n",
        "        if visualize:\n",
        "            print(f'--------Result save on {output_path}--------')\n",
        "            visualize_inference_on_video(video_source, dict_pred, output_path, fps,T,scale)\n",
        "    return dict_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loads checkpoint by local backend from path: rtmo-m_16xb16-600e_coco-640x640-6f4e0306_20231211.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# inferencer = MMPoseInferencer('human')\n",
        "config_file = 'configs/body_2d_keypoint/rtmo/coco/rtmo-m_16xb16-600e_coco-640x640.py'\n",
        "checkpoint_file = 'rtmo-m_16xb16-600e_coco-640x640-6f4e0306_20231211.pth'\n",
        "\n",
        "# build the inferencer with model config path and checkpoint path/URL\n",
        "inferencer = MMPoseInferencer(\n",
        "    pose2d=config_file,\n",
        "    pose2d_weights=checkpoint_file,\n",
        "    device= 'cpu'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1nY7jezbQzLf_SepALLpzafGjL0YShmjS\n",
            "From (redirected): https://drive.google.com/uc?id=1nY7jezbQzLf_SepALLpzafGjL0YShmjS&confirm=t&uuid=b072d212-709d-443d-a5ff-9ee145eed4ec\n",
            "To: /teamspace/studios/this_studio/mmpose/Weights.zip\n",
            "100%|████████████████████████████████████████| 124M/124M [00:04<00:00, 31.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1nY7jezbQzLf_SepALLpzafGjL0YShmjS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  Weights.zip\n",
            "   creating: Weights/\n",
            "  inflating: Weights/best_model_c3d_both.pth  \n",
            "  inflating: Weights/best_model_c3d_joint.pth  \n",
            "  inflating: Weights/best_model_c3d_limb.pth  \n",
            "  inflating: Weights/best_model_x3d_both.pth  \n",
            "  inflating: Weights/best_model_x3d_joint.pth  \n",
            "  inflating: Weights/best_model_x3d_limb.pth  \n",
            "  inflating: Weights/rtmo-coco-640x640.pth  \n"
          ]
        }
      ],
      "source": [
        "!unzip Weights.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config file written to: model_config.py\n"
          ]
        }
      ],
      "source": [
        "# Script to write the config file in Jupyter Notebook cell\n",
        "\n",
        "config_content = \"\"\"\n",
        "# model = dict(\n",
        "#     type='Recognizer3D',\n",
        "#     backbone=dict(\n",
        "#         type='X3D',\n",
        "#         gamma_d=1,\n",
        "#         in_channels=17,\n",
        "#         base_channels=24,\n",
        "#         num_stages=3,\n",
        "#         se_ratio=None,\n",
        "#         use_swish=False,\n",
        "#         stage_blocks=(2, 5, 3),\n",
        "#         spatial_strides=(2, 2, 2)),\n",
        "#     cls_head=dict(\n",
        "#         type='I3DHead',\n",
        "#         in_channels=216,\n",
        "#         num_classes=60,\n",
        "#         dropout=0.5),\n",
        "#     test_cfg=dict(average_clips='prob'))\n",
        "model = dict(\n",
        "    type='Recognizer3D',\n",
        "    backbone=dict(\n",
        "        type='C3D',\n",
        "        in_channels=17,\n",
        "        base_channels=32,\n",
        "        num_stages=3,\n",
        "        temporal_downsample=False),\n",
        "    cls_head=dict(\n",
        "        type='I3DHead',\n",
        "        in_channels=256,\n",
        "        num_classes=2,\n",
        "        dropout=0.5),\n",
        "    test_cfg=dict(average_clips='prob'))\n",
        "\n",
        "# lr_new =  lr / (num_gpu * batch_size_per_gpu) (lr=0.4 | 8 gpus)\n",
        "optimizer=dict(\n",
        "    type='SGD',\n",
        "    lr=0.00625,\n",
        "    momentum=0.9,\n",
        "    weight_decay=0.0003)\n",
        "\n",
        "optimizer_config=dict(\n",
        "    grad_clip=dict(max_norm=40, norm_type=2))\n",
        "\n",
        "# Learning policy\n",
        "lr_config=dict(\n",
        "    policy='CosineAnnealing',\n",
        "    by_epoch=False,\n",
        "    min_lr=0)\n",
        "\n",
        "total_epochs=24\n",
        "\n",
        "checkpoint_config=dict(\n",
        "    interval=1)\n",
        "# Dataset configuration\n",
        "dataset = dict(\n",
        "    type='HeatmapVideoDataset',\n",
        "    root_dir='./Heatmap_Dataset',\n",
        "    num_frames=64,\n",
        "    heatmap_type='joint',\n",
        "    heatmap_size=(17, 56, 56),\n",
        "    test_size = 0.2,\n",
        "    batch_size = 32\n",
        ")\n",
        "\n",
        "transform = dict(\n",
        "  type='DropKeypoint',\n",
        "  drop_prob=0.125\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Specify the file path and name\n",
        "config_file_path = 'model_config.py'\n",
        "\n",
        "# Write the content to the file\n",
        "with open(config_file_path, 'w') as f:\n",
        "    f.write(config_content)\n",
        "\n",
        "print(f\"Config file written to: {config_file_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import load_config, build_model\n",
        "config = load_config(\"model_config.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cfg = config.model\n",
        "model = build_model(model_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Sử dụng {torch.cuda.device_count()} GPUs\")\n",
        "# model = nn.DataParallel(model)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"./Weights/best_model_c3d_joint.pth\",map_location=torch.device('cpu')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recognizer3D(\n",
            "  (backbone): C3D(\n",
            "    (conv1a): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): Conv3d(17, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pool1): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)\n",
            "    (conv2a): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pool2): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)\n",
            "    (conv3a): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (conv3b): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (pool3): AvgPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0)\n",
            "    (conv4a): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (conv4b): ConvBlock(\n",
            "      (block): Sequential(\n",
            "        (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (cls_head): I3DHead(\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "    (fc_cls): Linear(in_features=256, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------Extracting keypoints and generate heatmap for joint--------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(384, 17, 56, 56) -> torch.Size([64, 17, 56, 56])\n",
            "--------Runing inference on heatmap--------\n",
            "[-0.30702993  0.28088185]\n",
            "--------Prediction--------\n",
            "Person ID 5 -> Prediction: ['Shoplifting']\n",
            "--------Result save on output_video.mp4--------\n"
          ]
        }
      ],
      "source": [
        "dict_pred = process_input(\n",
        "    model = model,\n",
        "    video_source='./Shoplifting_Dataset/shoplifting/shop_lifter_0.mp4',\n",
        "    inferencer=inferencer,\n",
        "    heatmap_size=(56, 56),\n",
        "    heatmap_type='joint', \n",
        "    visualize=True, \n",
        "    output_path='output_video.mp4',\n",
        "    fps = 24.75,\n",
        "    min_T = 64,\n",
        "    T = 64,\n",
        "    scale = 0,\n",
        "    method = 'interpolate'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
